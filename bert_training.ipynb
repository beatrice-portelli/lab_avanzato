{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from utils import (InputExample, InputFeatures, _truncate_seq_pair, Args,\n",
    "                      convert_examples_to_features, load_and_cache_examples, train, evaluate)\n",
    "\n",
    "from pytorch_transformers import (\n",
    "    WEIGHTS_NAME, BertConfig,\n",
    "    BertForSequenceClassification,\n",
    "    BertTokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/HDD/bportelli/lab_avanzato/models/\n",
      "/mnt/HDD/bportelli/lab_avanzato/models/bert-base-multilingual-uncased_small/Chapter/\n",
      "/mnt/HDD/bportelli/lab_avanzato/models/bert-base-multilingual-uncased_small/Chapter/Estimators/\n",
      "/mnt/HDD/bportelli/lab_avanzato/models/bert-base-multilingual-uncased_small/Chapter/Training/\n",
      "/mnt/HDD/bportelli/lab_avanzato/models/bert-base-multilingual-uncased_small/Chapter/Predictions/\n"
     ]
    }
   ],
   "source": [
    "# All important paths and constants\n",
    "\n",
    "small = False\n",
    "\n",
    "folds_number = 10\n",
    "\n",
    "args = Args()\n",
    "aggregation_level = \"Chapter\"\n",
    "\n",
    "main_dir = \"/mnt/HDD/bportelli/lab_avanzato\"\n",
    "\n",
    "original_data_path = \"/mnt/HDD/bportelli/lab_avanzato/beatrice.pkl\"\n",
    "\n",
    "diagnosis_df_preprocessed_serialized = main_dir+\"/input_df_dropped{}.pkl\".format(\"_small\" if small else \"\")\n",
    "models_path = main_dir+\"/models/\"\n",
    "model_name = args.model_name_or_path + \"_small\" if small else args.model_name_or_path\n",
    "model_directory = \"{}{}/{}/\".format(models_path, model_name, aggregation_level)\n",
    "model_directory_estimators = \"{}{}/{}/Estimators/\".format(models_path, model_name, aggregation_level)\n",
    "model_directory_training = \"{}{}/{}/Training/\".format(models_path, model_name, aggregation_level)\n",
    "model_directory_predictions = \"{}{}/{}/Predictions/\".format(models_path, model_name, aggregation_level)\n",
    "\n",
    "if not os.path.exists(models_path):\n",
    "    os.mkdir(models_path)\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory, exist_ok=True)\n",
    "if not os.path.exists(model_directory_estimators):\n",
    "    os.mkdir(model_directory_estimators)\n",
    "if not os.path.exists(model_directory_training):\n",
    "    os.mkdir(model_directory_training)\n",
    "if not os.path.exists(model_directory_predictions):\n",
    "    os.mkdir(model_directory_predictions)\n",
    "\n",
    "all_features_path = model_directory+\"all_features.pkl\"\n",
    "all_label_codes_path = model_directory+\"all_label_codes.pkl\"\n",
    "label_map_path = model_directory+\"label_map.pkl\"\n",
    "evaluation_path = model_directory+\"evaluation_results.pkl\"\n",
    "\n",
    "print(models_path)\n",
    "print(model_directory)\n",
    "print(model_directory_estimators)\n",
    "print(model_directory_training)\n",
    "print(model_directory_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataframe\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Leaf</th>\n",
       "      <th>Category</th>\n",
       "      <th>Block</th>\n",
       "      <th>Chapter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RIF. TRAUMA AL 3° DITO DELLA MANO SN, RISALENT...</td>\n",
       "      <td>9249</td>\n",
       "      <td>924</td>\n",
       "      <td>17.10</td>\n",
       "      <td>CH_17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RIFERISCE TRAUMA EMICOSTATO DX IN SEGUITO A PD...</td>\n",
       "      <td>78002</td>\n",
       "      <td>780</td>\n",
       "      <td>16.1</td>\n",
       "      <td>CH_16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RISCONRO DI HB 7.5 STENOSI PILORICA DI NDD</td>\n",
       "      <td>78900</td>\n",
       "      <td>789</td>\n",
       "      <td>16.1</td>\n",
       "      <td>CH_16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SPALLA SX TRAUMA CONTUSIVO EMICOSTATO SIN</td>\n",
       "      <td>9249</td>\n",
       "      <td>924</td>\n",
       "      <td>17.10</td>\n",
       "      <td>CH_17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DOLORE E BRUCIORE ALLA BOCCA DELLO STOMACO DA ...</td>\n",
       "      <td>07999</td>\n",
       "      <td>079</td>\n",
       "      <td>1.9</td>\n",
       "      <td>CH_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text   Leaf Category  Block  \\\n",
       "0  RIF. TRAUMA AL 3° DITO DELLA MANO SN, RISALENT...   9249      924  17.10   \n",
       "1  RIFERISCE TRAUMA EMICOSTATO DX IN SEGUITO A PD...  78002      780   16.1   \n",
       "2         RISCONRO DI HB 7.5 STENOSI PILORICA DI NDD  78900      789   16.1   \n",
       "3          SPALLA SX TRAUMA CONTUSIVO EMICOSTATO SIN   9249      924  17.10   \n",
       "4  DOLORE E BRUCIORE ALLA BOCCA DELLO STOMACO DA ...  07999      079    1.9   \n",
       "\n",
       "  Chapter  \n",
       "0   CH_17  \n",
       "1   CH_16  \n",
       "2   CH_16  \n",
       "3   CH_17  \n",
       "4    CH_1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(original_data_path, \"rb\") as o:\n",
    "    input_df = pickle.load(o)\n",
    "\n",
    "print(\"Original dataframe\")\n",
    "input_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative/Absolute frequencies for Chapters\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chapter</th>\n",
       "      <th>Frequency-Absolute</th>\n",
       "      <th>Frequency-Relative</th>\n",
       "      <th>Frequency-Relative(%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CH_17</td>\n",
       "      <td>114509</td>\n",
       "      <td>0.273845</td>\n",
       "      <td>27.384474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CH_16</td>\n",
       "      <td>84664</td>\n",
       "      <td>0.202471</td>\n",
       "      <td>20.247134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CH_6</td>\n",
       "      <td>42548</td>\n",
       "      <td>0.101752</td>\n",
       "      <td>10.175223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CH_1</td>\n",
       "      <td>38956</td>\n",
       "      <td>0.093162</td>\n",
       "      <td>9.316207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CH_7</td>\n",
       "      <td>32222</td>\n",
       "      <td>0.077058</td>\n",
       "      <td>7.705792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CH_13</td>\n",
       "      <td>22862</td>\n",
       "      <td>0.054674</td>\n",
       "      <td>5.467377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CH_8</td>\n",
       "      <td>21481</td>\n",
       "      <td>0.051371</td>\n",
       "      <td>5.137115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CH_9</td>\n",
       "      <td>18301</td>\n",
       "      <td>0.043766</td>\n",
       "      <td>4.376628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CH_10</td>\n",
       "      <td>10955</td>\n",
       "      <td>0.026199</td>\n",
       "      <td>2.619854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CH_12</td>\n",
       "      <td>10382</td>\n",
       "      <td>0.024828</td>\n",
       "      <td>2.482823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CH_5</td>\n",
       "      <td>9167</td>\n",
       "      <td>0.021923</td>\n",
       "      <td>2.192260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CH_3</td>\n",
       "      <td>5354</td>\n",
       "      <td>0.012804</td>\n",
       "      <td>1.280393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CH_4</td>\n",
       "      <td>3116</td>\n",
       "      <td>0.007452</td>\n",
       "      <td>0.745182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>CH_2</td>\n",
       "      <td>1704</td>\n",
       "      <td>0.004075</td>\n",
       "      <td>0.407506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>CH_11</td>\n",
       "      <td>1401</td>\n",
       "      <td>0.003350</td>\n",
       "      <td>0.335045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CH_14</td>\n",
       "      <td>445</td>\n",
       "      <td>0.001064</td>\n",
       "      <td>0.106420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>CH_15</td>\n",
       "      <td>86</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.020567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Chapter  Frequency-Absolute  Frequency-Relative  Frequency-Relative(%)\n",
       "0    CH_17              114509            0.273845              27.384474\n",
       "1    CH_16               84664            0.202471              20.247134\n",
       "2     CH_6               42548            0.101752              10.175223\n",
       "3     CH_1               38956            0.093162               9.316207\n",
       "4     CH_7               32222            0.077058               7.705792\n",
       "5    CH_13               22862            0.054674               5.467377\n",
       "6     CH_8               21481            0.051371               5.137115\n",
       "7     CH_9               18301            0.043766               4.376628\n",
       "8    CH_10               10955            0.026199               2.619854\n",
       "9    CH_12               10382            0.024828               2.482823\n",
       "10    CH_5                9167            0.021923               2.192260\n",
       "11    CH_3                5354            0.012804               1.280393\n",
       "12    CH_4                3116            0.007452               0.745182\n",
       "13    CH_2                1704            0.004075               0.407506\n",
       "14   CH_11                1401            0.003350               0.335045\n",
       "15   CH_14                 445            0.001064               0.106420\n",
       "16   CH_15                  86            0.000206               0.020567"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Let us see how diagnosis are distributed among each:\n",
    "# - Leaf\n",
    "# - Category\n",
    "# - Block\n",
    "# - Chapter\n",
    "\n",
    "def compute_relative_frequency(dataframe, label):\n",
    "    dataframe_freq = dataframe[label].value_counts().to_frame().reset_index().rename(columns={'index':label, label: 'Frequency-Absolute'})\n",
    "    dataframe_freq[\"Frequency-Relative\"] = dataframe_freq[\"Frequency-Absolute\"].div(len(dataframe))\n",
    "    dataframe_freq[\"Frequency-Relative(%)\"] = dataframe_freq[\"Frequency-Absolute\"].div(len(dataframe))*100\n",
    "    return dataframe_freq\n",
    "\n",
    "diagnosis_df_annotated = input_df\n",
    "\n",
    "df_leaf_freq = compute_relative_frequency(diagnosis_df_annotated, \"Leaf\")\n",
    "df_category_freq = compute_relative_frequency(diagnosis_df_annotated, \"Category\")\n",
    "df_block_freq = compute_relative_frequency(diagnosis_df_annotated, \"Block\")\n",
    "df_chapter_freq = compute_relative_frequency(diagnosis_df_annotated, \"Chapter\")\n",
    "\n",
    "print(\"Relative/Absolute frequencies for Chapters\")\n",
    "df_chapter_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length before: 10\n",
      "Dataframe preprocessed loaded from path: /mnt/HDD/bportelli/lab_avanzato/input_df_dropped_small.pkl\n",
      "Length after: 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Leaf</th>\n",
       "      <th>Category</th>\n",
       "      <th>Block</th>\n",
       "      <th>Chapter</th>\n",
       "      <th>Text-Processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RIF. TRAUMA AL 3° DITO DELLA MANO SN, RISALENT...</td>\n",
       "      <td>9249</td>\n",
       "      <td>924</td>\n",
       "      <td>17.10</td>\n",
       "      <td>CH_17</td>\n",
       "      <td>RIF. TRAUMA AL 3  DITO DELLA MANO SN, RISALENT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RIFERISCE TRAUMA EMICOSTATO DX IN SEGUITO A PD...</td>\n",
       "      <td>78002</td>\n",
       "      <td>780</td>\n",
       "      <td>16.1</td>\n",
       "      <td>CH_16</td>\n",
       "      <td>RIFERISCE TRAUMA EMICOSTATO DX IN SEGUITO A PD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RISCONRO DI HB 7.5 STENOSI PILORICA DI NDD</td>\n",
       "      <td>78900</td>\n",
       "      <td>789</td>\n",
       "      <td>16.1</td>\n",
       "      <td>CH_16</td>\n",
       "      <td>RISCONRO DI HB 7.5 STENOSI PILORICA DI NDD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SPALLA SX TRAUMA CONTUSIVO EMICOSTATO SIN</td>\n",
       "      <td>9249</td>\n",
       "      <td>924</td>\n",
       "      <td>17.10</td>\n",
       "      <td>CH_17</td>\n",
       "      <td>SPALLA SX TRAUMA CONTUSIVO EMICOSTATO SIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DOLORE E BRUCIORE ALLA BOCCA DELLO STOMACO DA ...</td>\n",
       "      <td>07999</td>\n",
       "      <td>079</td>\n",
       "      <td>1.9</td>\n",
       "      <td>CH_1</td>\n",
       "      <td>DOLORE E BRUCIORE ALLA BOCCA DELLO STOMACO DA ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text   Leaf Category  Block  \\\n",
       "0  RIF. TRAUMA AL 3° DITO DELLA MANO SN, RISALENT...   9249      924  17.10   \n",
       "1  RIFERISCE TRAUMA EMICOSTATO DX IN SEGUITO A PD...  78002      780   16.1   \n",
       "2         RISCONRO DI HB 7.5 STENOSI PILORICA DI NDD  78900      789   16.1   \n",
       "3          SPALLA SX TRAUMA CONTUSIVO EMICOSTATO SIN   9249      924  17.10   \n",
       "4  DOLORE E BRUCIORE ALLA BOCCA DELLO STOMACO DA ...  07999      079    1.9   \n",
       "\n",
       "  Chapter                                     Text-Processed  \n",
       "0   CH_17  RIF. TRAUMA AL 3  DITO DELLA MANO SN, RISALENT...  \n",
       "1   CH_16  RIFERISCE TRAUMA EMICOSTATO DX IN SEGUITO A PD...  \n",
       "2   CH_16         RISCONRO DI HB 7.5 STENOSI PILORICA DI NDD  \n",
       "3   CH_17          SPALLA SX TRAUMA CONTUSIVO EMICOSTATO SIN  \n",
       "4    CH_1  DOLORE E BRUCIORE ALLA BOCCA DELLO STOMACO DA ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.exists(diagnosis_df_preprocessed_serialized):\n",
    "    diagnosis_df_annotated = pd.read_pickle(diagnosis_df_preprocessed_serialized)\n",
    "    print(\"Length before: {}\".format(len(diagnosis_df_annotated)))\n",
    "    print(\"Dataframe preprocessed loaded from path: {}\".format(diagnosis_df_preprocessed_serialized))\n",
    "    print(\"Length after: {}\".format(len(diagnosis_df_annotated)))\n",
    "else:\n",
    "    for index, row in tqdm(diagnosis_df_annotated.iterrows(), desc=\"Processed\", total=len(diagnosis_df_annotated)):\n",
    "            text = row[\"Text\"]\n",
    "            # PRELIMINARY OPERATIONS\n",
    "            # Removing \\n, \\r and ° chars\n",
    "            text = text.replace('\\r', ' ')\n",
    "            text = text.replace('\\n', ' ')\n",
    "            text = text.replace('°', ' ')\n",
    "            # TOKENIZATION \n",
    "            # tokens = nlp(text)\n",
    "            # PUNCTUATION REMOVAL\n",
    "            # tokens = [token for token in tokens if token.pos_ != 'PUNCT']\n",
    "            # LEMMATIZATION\n",
    "            # lemmas = [t.lemma_ for t in tokens]\n",
    "            # SAVING PRE PROCESSED TEXT\n",
    "            # lemmatized_text = ' '.join(lemmas)\n",
    "            lemmatized_text = text\n",
    "            diagnosis_df_annotated.at[index,'Text-Processed'] = lemmatized_text\n",
    "\n",
    "    df_frequency = compute_relative_frequency(diagnosis_df_annotated, aggregation_level)\n",
    "    print(\"Filtering entries for a number of folds equal to: {}\".format(folds_number))\n",
    "    print(\"Length before: {}\".format(len(diagnosis_df_annotated)))\n",
    "    for index, row in df_frequency.iterrows():\n",
    "        code = row[aggregation_level]\n",
    "        frequency = row[\"Frequency-Absolute\"]\n",
    "        if frequency < folds_number:\n",
    "            diagnosis_df_annotated.drop(diagnosis_df_annotated[diagnosis_df_annotated[aggregation_level] == code].index,inplace=True)\n",
    "    \n",
    "    if small:\n",
    "        diagnosis_df_annotated = diagnosis_df_annotated.iloc[:10,]\n",
    "    \n",
    "    print(\"Length after: {}\".format(len(diagnosis_df_annotated)))\n",
    "\n",
    "    diagnosis_df_annotated.to_pickle(diagnosis_df_preprocessed_serialized)\n",
    "    print(\"Dataframe preprocessed saved at path: {}\".format(diagnosis_df_preprocessed_serialized))\n",
    "\n",
    "diagnosis_df_annotated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features loaded from path: /mnt/HDD/bportelli/lab_avanzato/models/bert-base-multilingual-uncased_small/Chapter/all_features.pkl\n",
      "InputFeatures(\"0\",..., \"0\")\n",
      "InputFeatures(\"1\",..., \"1\")\n",
      "InputFeatures(\"2\",..., \"1\")\n"
     ]
    }
   ],
   "source": [
    "label_list = input_df[aggregation_level].unique().tolist()\n",
    "num_labels = len(label_list)\n",
    "\n",
    "config_class, model_class, tokenizer_class = BertConfig, BertForSequenceClassification, BertTokenizer\n",
    "config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path, num_labels=num_labels)\n",
    "tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name if args.tokenizer_name else args.model_name_or_path, do_lower_case=args.do_lower_case)\n",
    "\n",
    "if os.path.exists(all_features_path):\n",
    "    with open(all_features_path, \"rb\") as o:\n",
    "        features = pickle.load(o)\n",
    "    print(\"All features loaded from path: {}\".format(all_features_path))\n",
    "else:\n",
    "    text_list = diagnosis_df_annotated[\"Text-Processed\"].values.tolist()\n",
    "\n",
    "    all_examples = []\n",
    "    for idx, text in tqdm(enumerate(text_list), desc=\"Creating examples\", total=len(diagnosis_df_annotated)):\n",
    "        example = InputExample(guid=idx, text_a=text, label=diagnosis_df_annotated.loc[idx][aggregation_level])\n",
    "        all_examples.append(example)\n",
    "\n",
    "    for i in range(3):\n",
    "        print(all_examples[i])\n",
    "\n",
    "    features = convert_examples_to_features(all_examples, label_list, args.max_seq_length, tokenizer,\n",
    "                cls_token=tokenizer.cls_token,\n",
    "                cls_token_segment_id=0,\n",
    "                sep_token=tokenizer.sep_token,\n",
    "                pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
    "                pad_token_segment_id=0,\n",
    "                args = args)\n",
    "    with open(all_features_path, \"wb\") as o:\n",
    "        pickle.dump(features, o)\n",
    "    print(\"All features saved at path: {}\".format(all_features_path))\n",
    "    \n",
    "for i in range(3):\n",
    "    print(features[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All labels loaded from path: /mnt/HDD/bportelli/lab_avanzato/models/bert-base-multilingual-uncased_small/Chapter/all_label_codes.pkl\n",
      "[0, 1, 1, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(all_label_codes_path):\n",
    "    with open(all_label_codes_path, \"rb\") as o:\n",
    "        all_label_codes = pickle.load(o)\n",
    "    with open(label_map_path, \"rb\") as o:\n",
    "        label_map = pickle.load(o)\n",
    "    print(\"All labels loaded from path: {}\".format(all_label_codes_path))\n",
    "\n",
    "else:\n",
    "    all_labels = diagnosis_df_annotated[aggregation_level].to_list()\n",
    "    label_map = {label : i for i, label in enumerate(label_list)}\n",
    "    all_label_codes = [label_map[l] for l in all_labels]\n",
    "\n",
    "    del all_labels\n",
    "    \n",
    "    with open(all_label_codes_path, \"wb\") as o:\n",
    "        pickle.dump(all_label_codes, o)\n",
    "    with open(label_map_path, \"wb\") as o:\n",
    "        pickle.dump(label_map, o)\n",
    "    print(\"All labels saved to path: {}\".format(all_label_codes_path))\n",
    "    \n",
    "print(all_label_codes[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(105879, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (classifier): Linear(in_features=768, out_features=17, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if torch.cuda.is_available() and args.use_cuda :\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and args.use_cuda else \"cpu\")\n",
    "args.device = device\n",
    "set_seed(args)\n",
    "\n",
    "model = model_class.from_pretrained(args.model_name_or_path, from_tf=bool('.ckpt' in args.model_name_or_path), config=config)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current model: bert-base-multilingual-uncased_small\n",
      "/mnt/HDD/bportelli/lab_avanzato/models/bert-base-multilingual-uncased_small/Chapter/\n",
      "/mnt/HDD/bportelli/lab_avanzato/models/bert-base-multilingual-uncased_small/Chapter/Estimators/\n",
      "/mnt/HDD/bportelli/lab_avanzato/models/bert-base-multilingual-uncased_small/Chapter/Training/\n"
     ]
    }
   ],
   "source": [
    "print(\"Current model: {}\".format(model_name))\n",
    "fold_counter = 1\n",
    "\n",
    "print(model_directory)\n",
    "print(model_directory_estimators)\n",
    "print(model_directory_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "codes = all_label_codes\n",
    "\n",
    "data = {}\n",
    "\n",
    "sk_fold = StratifiedKFold(n_splits=folds_number)\n",
    "\n",
    "if args.do_train:\n",
    "\n",
    "    for training_indexes, test_indexes in sk_fold.split(X=np.zeros(len(codes)), y=codes):\n",
    "        print(\"Processing fold: {}\".format(fold_counter))\n",
    "        # Merging together rows of the CSR matrix selected to be the training set of the current fold\n",
    "        training_data = []\n",
    "        training_labels = []\n",
    "        test_data = []\n",
    "        test_labels = []\n",
    "        for index in training_indexes:\n",
    "            training_data.append(features[index])\n",
    "            training_labels.append(all_label_codes[index])\n",
    "        for index in test_indexes:\n",
    "            test_data.append(features[index])\n",
    "            test_labels.append(all_label_codes[index])\n",
    "\n",
    "        # Convert to Tensors and build dataset\n",
    "        all_unique_ids = torch.tensor([f.unique_id for f in training_data], dtype=torch.long) \n",
    "        all_input_ids = torch.tensor([f.input_ids for f in training_data], dtype=torch.long)\n",
    "        all_input_mask = torch.tensor([f.input_mask for f in training_data], dtype=torch.long)\n",
    "        all_segment_ids = torch.tensor([f.segment_ids for f in training_data], dtype=torch.long)\n",
    "        all_label_ids = torch.tensor([f.label_id for f in training_data], dtype=torch.long)\n",
    "\n",
    "        train_dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids, all_unique_ids)\n",
    "\n",
    "        train(args, train_dataset, model, tokenizer)\n",
    "\n",
    "        # Some serialization and things\n",
    "\n",
    "        data[\"Fold-{}\".format(fold_counter)] = {}\n",
    "        data[\"Fold-{}\".format(fold_counter)][\"Training-Data\"] = training_data\n",
    "        data[\"Fold-{}\".format(fold_counter)][\"Training-Labels\"] = training_labels\n",
    "        data[\"Fold-{}\".format(fold_counter)][\"Test-Data\"] = test_data\n",
    "        data[\"Fold-{}\".format(fold_counter)][\"Test-Labels\"] = test_labels\n",
    "        model_output_dir = \"{}{}-Fold-{}-Model\".format(model_directory_estimators, model_name, fold_counter)\n",
    "        data_serialized = \"{}{}-Fold-{}-Data.pkl\".format(model_directory_training, model_name, fold_counter)\n",
    "\n",
    "        if not os.path.exists(model_output_dir):\n",
    "            os.makedirs(model_output_dir)\n",
    "\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model\n",
    "        model_to_save.save_pretrained(model_output_dir)\n",
    "        torch.save(args, os.path.join(model_output_dir, 'training_args.bin'))\n",
    "        tokenizer.save_pretrained(model_output_dir)\n",
    "\n",
    "        print(\"Model serialized at path: {}\".format(model_output_dir))\n",
    "\n",
    "        if not os.path.exists(data_serialized):\n",
    "            with open(data_serialized, \"wb\") as output_file:\n",
    "                pickle.dump(data, output_file)\n",
    "                print(\"Training data serialized at path: {}\".format(data_serialized))\n",
    "\n",
    "        # The fold is processed        \n",
    "\n",
    "        fold_counter+=1\n",
    "\n",
    "    # The training is completed\n",
    "\n",
    "    print(\"{} training done.\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 2/2 [00:00<00:00, 29.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation  *****\n",
      "  Num examples = %d 7\n",
      "  Batch size = %d 4\n",
      "Prediction serialized at path: /mnt/HDD/bportelli/lab_avanzato/models/bert-base-multilingual-uncased_small/Chapter/Predictions/bert-base-multilingual-uncased_small-Fold-1-Prediction.pkl\n",
      "All-Predictions serialized at path: /mnt/HDD/bportelli/lab_avanzato/models/bert-base-multilingual-uncased_small/Chapter/Predictions/bert-base-multilingual-uncased_small-Fold-1-Prediction.pkl\n",
      "Processing fold: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 33.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation  *****\n",
      "  Num examples = %d 3\n",
      "  Batch size = %d 4\n",
      "Prediction serialized at path: /mnt/HDD/bportelli/lab_avanzato/models/bert-base-multilingual-uncased_small/Chapter/Predictions/bert-base-multilingual-uncased_small-Fold-2-Prediction.pkl\n",
      "All-Predictions serialized at path: /mnt/HDD/bportelli/lab_avanzato/models/bert-base-multilingual-uncased_small/Chapter/Predictions/bert-base-multilingual-uncased_small-Fold-2-Prediction.pkl\n",
      "bert-base-multilingual-uncased_small predictions done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if args.do_eval:\n",
    "\n",
    "    for fold_counter in range(1, folds_number+1):\n",
    "        print(\"Processing fold: {}\".format(fold_counter))\n",
    "\n",
    "        model_output_dir = \"{}{}-Fold-{}-Model\".format(model_directory_estimators, model_name, fold_counter)\n",
    "        data_serialized = \"{}{}-Fold-{}-Data.pkl\".format(model_directory_training, model_name, fold_counter)\n",
    "        prediction_serialized = \"{}{}-Fold-{}-Prediction.pkl\".format(model_directory_predictions, model_name, fold_counter)\n",
    "        all_predictions_serialized = \"{}{}-Fold-{}-All-Predictions.pkl\".format(model_directory_predictions, model_name, fold_counter)\n",
    "\n",
    "        model = model_class.from_pretrained(model_output_dir)\n",
    "        tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n",
    "        model.to(args.device)\n",
    "\n",
    "        with open(data_serialized, \"rb\") as input_file:\n",
    "            training_data = pickle.load(input_file)\n",
    "\n",
    "        test_data = training_data[\"Fold-{}\".format(fold_counter)][\"Test-Data\"]\n",
    "\n",
    "        # Actual prediction\n",
    "\n",
    "        # Convert to Tensors and build dataset\n",
    "        all_unique_ids = torch.tensor([f.unique_id for f in test_data], dtype=torch.long) \n",
    "        all_input_ids = torch.tensor([f.input_ids for f in test_data], dtype=torch.long)\n",
    "        all_input_mask = torch.tensor([f.input_mask for f in test_data], dtype=torch.long)\n",
    "        all_segment_ids = torch.tensor([f.segment_ids for f in test_data], dtype=torch.long)\n",
    "        all_label_ids = torch.tensor([f.label_id for f in test_data], dtype=torch.long)\n",
    "\n",
    "        test_dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids, all_unique_ids)\n",
    "\n",
    "        prediction, all_predictions = evaluate(args, test_dataset, model, tokenizer)\n",
    "\n",
    "        # Some serialization and things\n",
    "\n",
    "        with open(prediction_serialized, \"wb\") as output_file:\n",
    "            pickle.dump(prediction, output_file)\n",
    "            print(\"Prediction serialized at path: {}\".format(prediction_serialized))\n",
    "        with open(all_predictions_serialized, \"wb\") as output_file:\n",
    "            pickle.dump(all_predictions, output_file)\n",
    "            print(\"All-Predictions serialized at path: {}\".format(prediction_serialized))\n",
    "\n",
    "    # The predictions are completed\n",
    "\n",
    "    print(\"{} predictions done.\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold: 1\n",
      "Processing fold: 2\n",
      "Evaluation results saved to path: /mnt/HDD/bportelli/lab_avanzato/models/bert-base-multilingual-uncased_small/Chapter/evaluation_results.pkl\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(real_labels, best_preds, all_preds):\n",
    "    acc_1 = (real_labels == best_preds).mean()\n",
    "    acc_3 = np.mean([1 if r in a[:3] else 0 for (r,a) in zip(real_labels, all_preds)])\n",
    "    acc_5 = np.mean([1 if r in a[:5] else 0 for (r,a) in zip(real_labels, all_preds)])\n",
    "    return acc_1, acc_3, acc_5\n",
    "\n",
    "results_df = pd.DataFrame(columns=[\"Fold\", \"Accuracy@1\", \"Accuracy@3\", \"Accuracy@5\"])\n",
    "\n",
    "for fold_counter in range(1, folds_number+1):\n",
    "    print(\"Processing fold: {}\".format(fold_counter))\n",
    "\n",
    "    data_serialized = \"{}{}-Fold-{}-Data.pkl\".format(model_directory_training, model_name, fold_counter)\n",
    "    prediction_serialized = \"{}{}-Fold-{}-Prediction.pkl\".format(model_directory_predictions, model_name, fold_counter)\n",
    "    all_predictions_serialized = \"{}{}-Fold-{}-All-Predictions.pkl\".format(model_directory_predictions, model_name, fold_counter)\n",
    "\n",
    "    with open(data_serialized, \"rb\") as input_file:\n",
    "        data = pickle.load(input_file)\n",
    "        \n",
    "    with open(prediction_serialized, \"rb\") as input_file:\n",
    "        best_prediction = pickle.load(input_file)\n",
    "        \n",
    "    with open(all_predictions_serialized, \"rb\") as input_file:\n",
    "        all_predictions = pickle.load(input_file)\n",
    "    \n",
    "    real_labels = data[\"Fold-{}\".format(fold_counter)][\"Test-Labels\"]\n",
    "    \n",
    "    acc_1, acc_3, acc_5 = compute_accuracy(real_labels, best_prediction, all_predictions)\n",
    "    results_df = results_df.append({\"Fold\": fold_counter,\n",
    "                       \"Accuracy@1\": acc_1,\n",
    "                       \"Accuracy@3\": acc_3,\n",
    "                       \"Accuracy@5\": acc_5}, ignore_index=True)\n",
    "\n",
    "results_df.to_pickle(evaluation_path)\n",
    "print(\"Evaluation results saved to path: {}\".format(evaluation_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
